{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5deeda1",
   "metadata": {},
   "source": [
    "Text generation pipeline using the pretrained distilbert model finetuned on the CVPR abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74a63228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.8849564790725708,\n",
       "  'token': 1012,\n",
       "  'token_str': '.',\n",
       "  'sequence': 'state of the art object detectors.'},\n",
       " {'score': 0.05219423398375511,\n",
       "  'token': 1025,\n",
       "  'token_str': ';',\n",
       "  'sequence': 'state of the art object detectors ;'}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_checkpoint=\"distilbert-base-uncased-finetuned-cvpr\"\n",
    "unmasker = pipeline(\"fill-mask\", model=model_checkpoint)\n",
    "unmasker(\"State of the art object detectors [MASK]\", top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0ff819",
   "metadata": {},
   "source": [
    "Punctuation are too likely, we need to filter them out. Import the model and the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "822936c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "model_checkpoint = \"distilbert-base-uncased-finetuned-cvpr\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8d01ab",
   "metadata": {},
   "source": [
    "Test on the masked text to check it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e23f6812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State of the art object detection detectors .\n",
      "State of the art object detection detectors ;\n",
      "State of the art object detection detectors :\n",
      "State of the art object detection detectors include\n",
      "State of the art object detection detectors are\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "text = \"State of the art object detection detectors [MASK]\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "token_logits = model(**inputs).logits\n",
    "\n",
    "# Find the location of [MASK] and extract its logits\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "# Pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(f\"{text.replace(tokenizer.mask_token, tokenizer.decode([token]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c90eb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State of the art object detection detectors include\n",
      "State of the art object detection detectors are\n",
      "State of the art object detection detectors have\n",
      "State of the art object detection detectors exist\n",
      "State of the art object detection detectors on\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "punc = \"! . , ' : ; - ( ) & | ?\"\n",
    "k2 = k + 12 # top predictions are all punc in the worst case\n",
    "top_topkens = torch.topk(mask_token_logits, k2, dim=1).indices[0].tolist()\n",
    "\n",
    "cnt = 0\n",
    "for token in top_topkens:\n",
    "    pred = tokenizer.decode([token])\n",
    "    if not (pred in punc):\n",
    "        print(f\"{text.replace(tokenizer.mask_token, pred)}\")\n",
    "        cnt += 1\n",
    "    if cnt == k: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f98cfa",
   "metadata": {},
   "source": [
    "Wrap this in a class method for model deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0ca3e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextFill:\n",
    "    def __init__(self, model_checkpoint=\"distilbert-base-uncased-finetuned-cvpr\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "        self.model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "        self.punc = \"!.,':;-()&|?\"\n",
    "        self.punc_len = len(self.punc)\n",
    "    \n",
    "    def _predict(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\")\n",
    "        token_logits = self.model(**inputs).logits\n",
    "\n",
    "        # Find the location of [MASK] and extract its logits\n",
    "        mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "        mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "\n",
    "        return mask_token_logits\n",
    "    \n",
    "    def _get_top_k(self, mask_token_logits, top_k):\n",
    "        k2 = top_k + self.punc_len\n",
    "        top_topkens = torch.topk(mask_token_logits, k2, dim=1).indices[0].tolist()\n",
    "        out = []\n",
    "        cnt = 0\n",
    "        for token in top_topkens:\n",
    "            pred = self.tokenizer.decode([token])\n",
    "            if not (pred in punc):\n",
    "                out.append(pred)\n",
    "                cnt += 1\n",
    "            if cnt == top_k: break\n",
    "        return out\n",
    "    \n",
    "    def fill_mask(self, text, top_k=10):\n",
    "        mask_token_logits = self._predict(text)\n",
    "        out = self._get_top_k(mask_token_logits, top_k)\n",
    "        return out\n",
    "\n",
    "tf = TextFill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17ee0e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['optimization', 'inference', 'training', '##ness', '##ly', 'estimation', 'learning', 'reconstruction', 'modeling', 'computation']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"To this end, we design a deep latent space deformation network that is directly parameterized by the kernel. \n",
    "          The network consists of three components: encoder, deformer, and decoder, where the deformer is specifically\n",
    "          meant to rectify the latent space representations of blurred images to a standard latent space, regardless of\n",
    "          the kernel. The deformation network is trained with a two-stage training scheme. We conduct extensive\n",
    "          experiments to confirm that our parametric model can adapt to drastically different blurring kernels and perform\n",
    "          robust [MASK].\"\"\" # original word: deblurring\n",
    "pred = tf.fill_mask(text, 10)\n",
    "print(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "5bb167dcc8e0362259233268f8aabb06284b5cd30446df487de294e1c7e8a6f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
